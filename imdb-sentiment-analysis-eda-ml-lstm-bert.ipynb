{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Dataset Information\n\nIMDB dataset contains 50K movie reviews for natural language processing i.e.  for binary sentiment classification. The dataset contains two columns - review and sentiment to perform the sentimental analysis.\n\n### Problem Statement\nCorrectly classify the positive and negative sentiments for IMDB reviews.\n\n### Overview\nIn this notebook, I perform cleaning on the dataset, exploratory data analysis and predicitive modelling using machine learning and deep learning algorithms. For machine learning I use Logistic Regression, Multinomial Naive Bayes, Linear SVM and XGBoost.\nFor Deep Learning, I use a combination of CNN+LSTM and BERT. The maximum accuracy achieved using BERT is around 90%.\n\n### References\nThis kernel includes some ideas and parts of code from the following references:\n1. https://www.kaggle.com/madz2000/sentiment-analysis-cleaning-eda-bert-88-acc/output\n2. https://www.kaggle.com/aditya6040/7-models-on-imdb-dataset-best-score-88-2\n3. https://www.kaggle.com/faressayah/sentiment-model-with-tensorflow-transformers\n4. https://www.kaggle.com/sravyaysk/imdb-movie-review-classification-using-tensorflow\n5. https://www.kaggle.com/colearninglounge/nlp-data-preprocessing-and-cleaning\n6. https://huggingface.co/transformers/master/custom_datasets.html\n\nPlease upvote if you like my work.Thank you all of you !","metadata":{}},{"cell_type":"markdown","source":"# **1. Importing Libraries**","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize\nfrom wordcloud import WordCloud,STOPWORDS\nfrom bs4 import BeautifulSoup\nimport re,string,unicodedata\n\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import LinearSVC\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.naive_bayes import GaussianNB, MultinomialNB\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, accuracy_score, confusion_matrix, plot_confusion_matrix, plot_roc_curve, plot_precision_recall_curve\nfrom xgboost.sklearn import XGBClassifier\n\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.layers import Dense,Input, Embedding,LSTM,Dropout,Conv1D, MaxPooling1D, GlobalMaxPooling1D,Dropout,Bidirectional,Flatten,BatchNormalization\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.utils import plot_model\nimport transformers\nimport tokenizers\n","metadata":{"execution":{"iopub.status.busy":"2021-07-23T19:28:27.060671Z","iopub.execute_input":"2021-07-23T19:28:27.061287Z","iopub.status.idle":"2021-07-23T19:28:29.95173Z","shell.execute_reply.started":"2021-07-23T19:28:27.061215Z","shell.execute_reply":"2021-07-23T19:28:29.950863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **2. Data Extraction and Cleaning**","metadata":{}},{"cell_type":"code","source":"data=pd.read_csv('../input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv')\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-23T19:28:29.962729Z","iopub.execute_input":"2021-07-23T19:28:29.963091Z","iopub.status.idle":"2021-07-23T19:28:30.523488Z","shell.execute_reply.started":"2021-07-23T19:28:29.963054Z","shell.execute_reply":"2021-07-23T19:28:30.522515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.info()","metadata":{"execution":{"iopub.status.busy":"2021-07-23T19:28:30.525371Z","iopub.execute_input":"2021-07-23T19:28:30.525756Z","iopub.status.idle":"2021-07-23T19:28:30.552449Z","shell.execute_reply.started":"2021-07-23T19:28:30.525715Z","shell.execute_reply":"2021-07-23T19:28:30.551427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.describe() #descriptive statistics","metadata":{"execution":{"iopub.status.busy":"2021-07-23T19:28:30.553959Z","iopub.execute_input":"2021-07-23T19:28:30.554367Z","iopub.status.idle":"2021-07-23T19:28:30.669689Z","shell.execute_reply.started":"2021-07-23T19:28:30.554328Z","shell.execute_reply":"2021-07-23T19:28:30.668885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"null_values = data.isnull().sum() #identifying missing values","metadata":{"execution":{"iopub.status.busy":"2021-07-23T19:28:30.670939Z","iopub.execute_input":"2021-07-23T19:28:30.671325Z","iopub.status.idle":"2021-07-23T19:28:30.685913Z","shell.execute_reply.started":"2021-07-23T19:28:30.671288Z","shell.execute_reply":"2021-07-23T19:28:30.684967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"null_values.index[0]","metadata":{"execution":{"iopub.status.busy":"2021-07-23T19:28:30.68745Z","iopub.execute_input":"2021-07-23T19:28:30.688102Z","iopub.status.idle":"2021-07-23T19:28:30.697262Z","shell.execute_reply.started":"2021-07-23T19:28:30.688065Z","shell.execute_reply":"2021-07-23T19:28:30.696234Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('There are {} missing values for {} and {} missing values for {}.'.format(null_values[0],null_values.index[0],null_values[1],null_values.index[1]))","metadata":{"execution":{"iopub.status.busy":"2021-07-23T19:28:30.698619Z","iopub.execute_input":"2021-07-23T19:28:30.699322Z","iopub.status.idle":"2021-07-23T19:28:30.706926Z","shell.execute_reply.started":"2021-07-23T19:28:30.699281Z","shell.execute_reply":"2021-07-23T19:28:30.705941Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_duplicates = data.duplicated().sum() #identify duplicates\nprint('There are {} duplicate reviews present in the dataset'.format(num_duplicates))","metadata":{"execution":{"iopub.status.busy":"2021-07-23T19:28:30.710903Z","iopub.execute_input":"2021-07-23T19:28:30.711618Z","iopub.status.idle":"2021-07-23T19:28:30.884443Z","shell.execute_reply.started":"2021-07-23T19:28:30.71158Z","shell.execute_reply":"2021-07-23T19:28:30.883514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#view duplicate reviews\nreview = data['review']\nduplicated_review = data[review.isin(review[review.duplicated()])].sort_values(\"review\")\nduplicated_review.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-23T19:28:30.886667Z","iopub.execute_input":"2021-07-23T19:28:30.887276Z","iopub.status.idle":"2021-07-23T19:28:30.913017Z","shell.execute_reply.started":"2021-07-23T19:28:30.887232Z","shell.execute_reply":"2021-07-23T19:28:30.912127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#drop duplicate reviews\ndata.drop_duplicates(inplace = True)","metadata":{"execution":{"iopub.status.busy":"2021-07-23T19:28:30.91438Z","iopub.execute_input":"2021-07-23T19:28:30.915057Z","iopub.status.idle":"2021-07-23T19:28:31.065952Z","shell.execute_reply.started":"2021-07-23T19:28:30.915016Z","shell.execute_reply":"2021-07-23T19:28:31.065156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('The dataset contains {} rows and {} columns after removing duplicates'.format(data.shape[0],data.shape[1]))","metadata":{"execution":{"iopub.status.busy":"2021-07-23T19:28:31.067085Z","iopub.execute_input":"2021-07-23T19:28:31.067581Z","iopub.status.idle":"2021-07-23T19:28:31.073133Z","shell.execute_reply.started":"2021-07-23T19:28:31.067543Z","shell.execute_reply":"2021-07-23T19:28:31.072087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stop = stopwords.words('english')\nwl = WordNetLemmatizer()","metadata":{"execution":{"iopub.status.busy":"2021-07-23T19:28:31.074544Z","iopub.execute_input":"2021-07-23T19:28:31.075145Z","iopub.status.idle":"2021-07-23T19:28:31.08576Z","shell.execute_reply.started":"2021-07-23T19:28:31.075108Z","shell.execute_reply":"2021-07-23T19:28:31.084787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \n           \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \n           \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \n           \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \n           \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \n           \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \n           \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \n           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\n           \"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \n           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\n           \"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \n           \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \n           \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \n           \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \n           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \n           \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \n           \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \n           \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \n           \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\n           \"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \n           \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \n           \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \n           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \n           \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \n           \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \n           \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \n           \"what'll\": \"what will\", \"what'll've\": \"what will have\",\"what're\": \"what are\",  \n           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \n           \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \n           \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \n           \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \n           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \n           \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\n           \"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \n           \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \n           \"you're\": \"you are\", \"you've\": \"you have\" }","metadata":{"execution":{"iopub.status.busy":"2021-07-23T19:28:31.08737Z","iopub.execute_input":"2021-07-23T19:28:31.08798Z","iopub.status.idle":"2021-07-23T19:28:31.105442Z","shell.execute_reply.started":"2021-07-23T19:28:31.087886Z","shell.execute_reply":"2021-07-23T19:28:31.104375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#function to clean data\ndef clean_text(text,lemmatize = True):\n    soup = BeautifulSoup(text, \"html.parser\") #remove html tags\n    text = soup.get_text()\n    text = ' '.join([mapping[t] if t in mapping else t for t in text.split(\" \")]) #expanding chatwords and contracts clearing contractions\n    emoji_clean= re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    text = emoji_clean.sub(r'',text)\n    text = re.sub(r'\\.(?=\\S)', '. ',text) #add space after full stop\n    text = re.sub(r'http\\S+', '', text) #remove urls\n    text = \"\".join([word.lower() for word in text if word not in string.punctuation]) #remove punctuation\n    #tokens = re.split('\\W+', text) #create tokens\n    if lemmatize:\n        text = \" \".join([wl.lemmatize(word) for word in text.split() if word not in stop and word.isalpha()]) #lemmatize\n    else:\n        text = \" \".join([word for word in text.split() if word not in stop and word.isalpha()]) \n    return text","metadata":{"execution":{"iopub.status.busy":"2021-07-23T19:28:31.106765Z","iopub.execute_input":"2021-07-23T19:28:31.107431Z","iopub.status.idle":"2021-07-23T19:28:31.118904Z","shell.execute_reply.started":"2021-07-23T19:28:31.10738Z","shell.execute_reply":"2021-07-23T19:28:31.117939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_copy = data.copy()","metadata":{"execution":{"iopub.status.busy":"2021-07-23T19:28:31.120298Z","iopub.execute_input":"2021-07-23T19:28:31.120939Z","iopub.status.idle":"2021-07-23T19:28:31.131677Z","shell.execute_reply.started":"2021-07-23T19:28:31.120901Z","shell.execute_reply":"2021-07-23T19:28:31.130787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['review']=data['review'].apply(clean_text,lemmatize = True)","metadata":{"execution":{"iopub.status.busy":"2021-07-23T19:28:31.133091Z","iopub.execute_input":"2021-07-23T19:28:31.133688Z","iopub.status.idle":"2021-07-23T19:29:53.992172Z","shell.execute_reply.started":"2021-07-23T19:28:31.133651Z","shell.execute_reply":"2021-07-23T19:29:53.991305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#converting target variable to numeric labels\ndata.sentiment = [ 1 if each == \"positive\" else 0 for each in data.sentiment]","metadata":{"execution":{"iopub.status.busy":"2021-07-23T19:29:53.993542Z","iopub.execute_input":"2021-07-23T19:29:53.993884Z","iopub.status.idle":"2021-07-23T19:29:54.032579Z","shell.execute_reply.started":"2021-07-23T19:29:53.993849Z","shell.execute_reply":"2021-07-23T19:29:54.031761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#after converting labels\ndata.head()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-07-23T19:29:54.033811Z","iopub.execute_input":"2021-07-23T19:29:54.034168Z","iopub.status.idle":"2021-07-23T19:29:54.044388Z","shell.execute_reply.started":"2021-07-23T19:29:54.034131Z","shell.execute_reply":"2021-07-23T19:29:54.043455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **3. Exploratory data analysis** ","metadata":{}},{"cell_type":"code","source":"### Count Plot\nsns.set(style = \"whitegrid\" , font_scale = 1.2)\nsns.countplot(data.sentiment,palette = ['green','red'],order = [1,0])\nplt.xticks(ticks = np.arange(2),labels = ['positive','negative'])\nplt.title('Target count for IMBD reviews')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-23T19:29:54.045935Z","iopub.execute_input":"2021-07-23T19:29:54.046287Z","iopub.status.idle":"2021-07-23T19:29:54.179936Z","shell.execute_reply.started":"2021-07-23T19:29:54.046249Z","shell.execute_reply":"2021-07-23T19:29:54.178951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Positive reviews are', (round(data['sentiment'].value_counts()[0])),'i.e.', round(data['sentiment'].value_counts()[0]/len(data) * 100,2), '% of the dataset')\nprint('Negative reviews are', (round(data['sentiment'].value_counts()[1])),'i.e.',round(data['sentiment'].value_counts()[1]/len(data) * 100,2), '% of the dataset')","metadata":{"execution":{"iopub.status.busy":"2021-07-23T19:29:54.181379Z","iopub.execute_input":"2021-07-23T19:29:54.181751Z","iopub.status.idle":"2021-07-23T19:29:54.195406Z","shell.execute_reply.started":"2021-07-23T19:29:54.181715Z","shell.execute_reply":"2021-07-23T19:29:54.194236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#word cloud for positive reviews\npositive_data = data[data.sentiment == 1]['review']\npositive_data_string = ' '.join(positive_data)\nplt.figure(figsize = (20,20))\nwc = WordCloud(max_words = 2000, width=1200, height=600,background_color=\"white\").generate(positive_data_string)\nplt.imshow(wc , interpolation = 'bilinear')\nplt.axis('off')\nplt.title('Word cloud for positive reviews',fontsize = 20)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-23T19:29:54.196919Z","iopub.execute_input":"2021-07-23T19:29:54.197269Z","iopub.status.idle":"2021-07-23T19:30:25.664312Z","shell.execute_reply.started":"2021-07-23T19:29:54.197234Z","shell.execute_reply":"2021-07-23T19:30:25.663369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#word cloud for negative reviews\nnegative_data = data[data.sentiment == 0]['review']\nnegative_data_string = ' '.join(negative_data)\nplt.figure(figsize = (20,20))\nwc = WordCloud(max_words = 2000, width=1200, height=600,background_color=\"white\").generate(negative_data_string)\nplt.imshow(wc , interpolation = 'bilinear')\nplt.axis('off')\nplt.title('Word cloud for negative reviews',fontsize = 20)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-23T19:30:25.665629Z","iopub.execute_input":"2021-07-23T19:30:25.665935Z","iopub.status.idle":"2021-07-23T19:30:57.652481Z","shell.execute_reply.started":"2021-07-23T19:30:25.665904Z","shell.execute_reply":"2021-07-23T19:30:57.651714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(12,8))\ntext_len=positive_data.str.len()\nax1.hist(text_len,color='green')\nax1.set_title('Positive Reviews')\nax1.set_xlabel('Number of Characters')\nax1.set_ylabel('Count')\ntext_len=negative_data.str.len()\nax2.hist(text_len,color='red')\nax2.set_title('Negative Reviews')\nax2.set_xlabel('Number of Characters')\nax2.set_ylabel('Count')\nfig.suptitle('Number of characters in texts')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-23T19:30:57.653602Z","iopub.execute_input":"2021-07-23T19:30:57.654019Z","iopub.status.idle":"2021-07-23T19:30:58.078455Z","shell.execute_reply.started":"2021-07-23T19:30:57.653986Z","shell.execute_reply":"2021-07-23T19:30:58.077531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(12,8))\n\ntext_len=positive_data.str.split().map(lambda x: len(x))\nax1.hist(text_len,color='green')\nax1.set_title('Positive Reviews')\nax1.set_xlabel('Number of Words')\nax1.set_ylabel('Count')\ntext_len=negative_data.str.split().map(lambda x: len(x))\nax2.hist(text_len,color='red')\nax2.set_title('Negative Reviews')\nax2.set_xlabel('Number of Words')\nax2.set_ylabel('Count')\nfig.suptitle('Number of words in texts')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-23T19:30:58.079778Z","iopub.execute_input":"2021-07-23T19:30:58.080246Z","iopub.status.idle":"2021-07-23T19:30:59.479667Z","shell.execute_reply.started":"2021-07-23T19:30:58.080204Z","shell.execute_reply":"2021-07-23T19:30:59.478658Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(20,10))\nword = positive_data.str.split().apply(lambda x : len(x) )\nsns.distplot(word, ax=ax1,color='green')\nax1.set_title('Positive Reviews')\nax1.set_xlabel('Number of words per review')\nword = negative_data.str.split().apply(lambda x :len(x) )\nsns.distplot(word,ax=ax2,color='red')\nax2.set_title('Negative Reviews')\nax2.set_xlabel('Number of words per review')\nfig.suptitle('Distribution of number of words per reviews')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-23T19:30:59.481114Z","iopub.execute_input":"2021-07-23T19:30:59.481482Z","iopub.status.idle":"2021-07-23T19:31:01.122294Z","shell.execute_reply.started":"2021-07-23T19:30:59.481442Z","shell.execute_reply":"2021-07-23T19:31:01.121303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(20,10))\nword = positive_data.str.split().apply(lambda x : [len(i) for i in x] )\nsns.distplot(word.map(lambda x: np.mean(x)), ax=ax1,color='green')\nax1.set_title('Positive Reviews')\nax1.set_xlabel('Average word length per review')\nword = negative_data.str.split().apply(lambda x : [len(i) for i in x] )\nsns.distplot(word.map(lambda x: np.mean(x)),ax=ax2,color='red')\nax2.set_title('Negative Reviews')\nax2.set_xlabel('Average word length per review')\nfig.suptitle('Distribution of average word length in each review')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-23T19:31:01.127828Z","iopub.execute_input":"2021-07-23T19:31:01.128181Z","iopub.status.idle":"2021-07-23T19:31:05.004693Z","shell.execute_reply.started":"2021-07-23T19:31:01.128145Z","shell.execute_reply":"2021-07-23T19:31:05.00355Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_corpus(text):\n    words = []\n    for i in text:\n        for j in i.split():\n            words.append(j.strip())\n    return words\ncorpus = get_corpus(data.review)\ncorpus[:5]","metadata":{"execution":{"iopub.status.busy":"2021-07-23T19:31:05.00739Z","iopub.execute_input":"2021-07-23T19:31:05.007902Z","iopub.status.idle":"2021-07-23T19:31:06.276678Z","shell.execute_reply.started":"2021-07-23T19:31:05.007858Z","shell.execute_reply":"2021-07-23T19:31:06.275647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from collections import Counter\ncounter = Counter(corpus)\nmost_common = counter.most_common(10)\nmost_common = pd.DataFrame(most_common,columns = ['corpus','countv'])\nmost_common","metadata":{"execution":{"iopub.status.busy":"2021-07-23T19:31:06.278057Z","iopub.execute_input":"2021-07-23T19:31:06.278454Z","iopub.status.idle":"2021-07-23T19:31:07.216428Z","shell.execute_reply.started":"2021-07-23T19:31:06.278403Z","shell.execute_reply":"2021-07-23T19:31:07.215402Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"most_common = most_common.sort_values('countv')","metadata":{"execution":{"iopub.status.busy":"2021-07-23T19:31:07.217918Z","iopub.execute_input":"2021-07-23T19:31:07.218288Z","iopub.status.idle":"2021-07-23T19:31:07.223353Z","shell.execute_reply.started":"2021-07-23T19:31:07.218251Z","shell.execute_reply":"2021-07-23T19:31:07.222394Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize =(10,10))\nplt.yticks(range(len(most_common)), list(most_common.corpus))\nplt.barh(range(len(most_common)), list(most_common.countv),align='center',color = 'blue')\nplt.title('Most common words in the dataset')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2021-07-23T19:31:07.224676Z","iopub.execute_input":"2021-07-23T19:31:07.22503Z","iopub.status.idle":"2021-07-23T19:31:07.591169Z","shell.execute_reply.started":"2021-07-23T19:31:07.224994Z","shell.execute_reply":"2021-07-23T19:31:07.590158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_ngrams(review, n, g):\n    vec = CountVectorizer(ngram_range=(g, g)).fit(review)\n    bag_of_words = vec.transform(review) #sparse matrix of count_vectorizer\n    sum_words = bag_of_words.sum(axis=0) #total number of words\n    sum_words = np.array(sum_words)[0].tolist() #convert to list\n    words_freq = [(word, sum_words[idx]) for word, idx in vec.vocabulary_.items()] #get word freqency for word location in count vec\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True) #key is used to perform sorting using word_freqency \n    return words_freq[:n]","metadata":{"execution":{"iopub.status.busy":"2021-07-23T19:31:07.592699Z","iopub.execute_input":"2021-07-23T19:31:07.593235Z","iopub.status.idle":"2021-07-23T19:31:07.601037Z","shell.execute_reply.started":"2021-07-23T19:31:07.593191Z","shell.execute_reply":"2021-07-23T19:31:07.600174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(30,15))\nuni_positive = get_ngrams(positive_data,20,1)\nuni_positive = dict(uni_positive)\ntemp = pd.DataFrame(list(uni_positive.items()), columns = [\"Common_words\" , 'Count'])\nsns.barplot(data = temp, x=\"Count\", y=\"Common_words\", orient='h',ax = ax1)\nax1.set_title('Positive reviews')\nuni_negative = get_ngrams(negative_data,20,1)\nuni_negative = dict(uni_negative)\ntemp = pd.DataFrame(list(uni_negative.items()), columns = [\"Common_words\" , 'Count'])\nsns.barplot(data = temp, x=\"Count\", y=\"Common_words\", orient='h',ax = ax2)\nax2.set_title('Negative reviews')\nfig.suptitle('Unigram analysis for positive and negative reviews')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2021-07-23T19:31:07.602335Z","iopub.execute_input":"2021-07-23T19:31:07.602901Z","iopub.status.idle":"2021-07-23T19:31:21.104338Z","shell.execute_reply.started":"2021-07-23T19:31:07.602858Z","shell.execute_reply":"2021-07-23T19:31:21.103375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(30,15))\nbi_positive = get_ngrams(positive_data,20,2)\nbi_positive = dict(bi_positive)\ntemp = pd.DataFrame(list(bi_positive.items()), columns = [\"Common_words\" , 'Count'])\nsns.barplot(data = temp, x=\"Count\", y=\"Common_words\", orient='h',ax = ax1)\nax1.set_title('Positive reviews')\nbi_negative = get_ngrams(negative_data,20,2)\nbi_negative = dict(bi_negative)\ntemp = pd.DataFrame(list(bi_negative.items()), columns = [\"Common_words\" , 'Count'])\nsns.barplot(data = temp, x=\"Count\", y=\"Common_words\", orient='h',ax = ax2)\nax2.set_title('Negative reviews')\nfig.suptitle('Bigram analysis for positive and negative reviews')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-23T19:31:21.105702Z","iopub.execute_input":"2021-07-23T19:31:21.106052Z","iopub.status.idle":"2021-07-23T19:32:01.200839Z","shell.execute_reply.started":"2021-07-23T19:31:21.106012Z","shell.execute_reply":"2021-07-23T19:32:01.199383Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(30,15))\ntri_positive = get_ngrams(positive_data,20,3)\ntri_positive = dict(tri_positive)\ntemp = pd.DataFrame(list(tri_positive.items()), columns = [\"Common_words\" , 'Count'])\nsns.barplot(data = temp, x=\"Count\", y=\"Common_words\", orient='h',ax = ax1)\nax1.set_title('Positive reviews')\ntri_negative = get_ngrams(negative_data,20,3)\ntri_negative = dict(tri_negative)\ntemp = pd.DataFrame(list(tri_negative.items()), columns = [\"Common_words\" , 'Count'])\nsns.barplot(data = temp, x=\"Count\", y=\"Common_words\", orient='h',ax = ax2)\nax2.set_title('Negative reviews')\nfig.suptitle('Trigram analysis for positive and negative reviews')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-23T19:32:01.202495Z","iopub.execute_input":"2021-07-23T19:32:01.202836Z","iopub.status.idle":"2021-07-23T19:32:52.809343Z","shell.execute_reply.started":"2021-07-23T19:32:01.202796Z","shell.execute_reply":"2021-07-23T19:32:52.808303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **4. Predictive Modelling using Machine Learning** ","metadata":{}},{"cell_type":"code","source":"#splitting into train and test\ntrain, test= train_test_split(data, test_size=0.2, random_state=42)\nXtrain, ytrain = train['review'], train['sentiment']\nXtest, ytest = test['review'], test['sentiment']","metadata":{"execution":{"iopub.status.busy":"2021-07-23T19:32:52.81113Z","iopub.execute_input":"2021-07-23T19:32:52.811955Z","iopub.status.idle":"2021-07-23T19:32:52.83153Z","shell.execute_reply.started":"2021-07-23T19:32:52.811909Z","shell.execute_reply":"2021-07-23T19:32:52.830529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Vectorizing data\n\ntfidf_vect = TfidfVectorizer() #tfidfVectorizer\nXtrain_tfidf = tfidf_vect.fit_transform(Xtrain)\nXtest_tfidf = tfidf_vect.transform(Xtest)\n\n\ncount_vect = CountVectorizer() # CountVectorizer\nXtrain_count = count_vect.fit_transform(Xtrain)\nXtest_count = count_vect.transform(Xtest)","metadata":{"execution":{"iopub.status.busy":"2021-07-23T19:32:52.833482Z","iopub.execute_input":"2021-07-23T19:32:52.834386Z","iopub.status.idle":"2021-07-23T19:33:07.416Z","shell.execute_reply.started":"2021-07-23T19:32:52.834344Z","shell.execute_reply":"2021-07-23T19:33:07.415108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Logistic Regression","metadata":{"execution":{"iopub.status.busy":"2021-07-23T07:02:58.677968Z","iopub.execute_input":"2021-07-23T07:02:58.678286Z","iopub.status.idle":"2021-07-23T07:03:02.363725Z","shell.execute_reply.started":"2021-07-23T07:02:58.678257Z","shell.execute_reply":"2021-07-23T07:03:02.362283Z"}}},{"cell_type":"code","source":"lr = LogisticRegression()\nlr.fit(Xtrain_tfidf,ytrain)\np1=lr.predict(Xtest_tfidf)\ns1=accuracy_score(ytest,p1)\nprint(\"Logistic Regression Accuracy :\", \"{:.2f}%\".format(100*s1))\nplot_confusion_matrix(lr, Xtest_tfidf, ytest,cmap = 'Blues')\nplt.grid(False)","metadata":{"execution":{"iopub.status.busy":"2021-07-23T19:33:07.417292Z","iopub.execute_input":"2021-07-23T19:33:07.41766Z","iopub.status.idle":"2021-07-23T19:33:10.693546Z","shell.execute_reply.started":"2021-07-23T19:33:07.417622Z","shell.execute_reply":"2021-07-23T19:33:10.692502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Multinomial Naive Bayes","metadata":{"execution":{"iopub.status.busy":"2021-07-23T07:03:18.719323Z","iopub.execute_input":"2021-07-23T07:03:18.71965Z","iopub.status.idle":"2021-07-23T07:03:19.008027Z","shell.execute_reply.started":"2021-07-23T07:03:18.71962Z","shell.execute_reply":"2021-07-23T07:03:19.007032Z"}}},{"cell_type":"code","source":"mnb= MultinomialNB()\nmnb.fit(Xtrain_tfidf,ytrain)\np2=mnb.predict(Xtest_tfidf)\ns2=accuracy_score(ytest,p2)\nprint(\"Multinomial Naive Bayes Classifier Accuracy :\", \"{:.2f}%\".format(100*s2))\nplot_confusion_matrix(mnb, Xtest_tfidf, ytest,cmap = 'Blues')\nplt.grid(False)","metadata":{"execution":{"iopub.status.busy":"2021-07-23T19:33:10.695246Z","iopub.execute_input":"2021-07-23T19:33:10.695611Z","iopub.status.idle":"2021-07-23T19:33:10.952148Z","shell.execute_reply.started":"2021-07-23T19:33:10.69557Z","shell.execute_reply":"2021-07-23T19:33:10.951006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Linear SVM","metadata":{"execution":{"iopub.status.busy":"2021-07-23T07:03:37.357687Z","iopub.execute_input":"2021-07-23T07:03:37.358139Z","iopub.status.idle":"2021-07-23T07:03:40.151133Z","shell.execute_reply.started":"2021-07-23T07:03:37.358096Z","shell.execute_reply":"2021-07-23T07:03:40.150309Z"}}},{"cell_type":"code","source":"linear_svc = LinearSVC(penalty='l2',loss = 'hinge')\nlinear_svc.fit(Xtrain_tfidf,ytrain)\np3=linear_svc.predict(Xtest_tfidf)\ns3=accuracy_score(ytest,p3)\nprint(\"Linear Support Vector Classifier Accuracy :\", \"{:.2f}%\".format(100*s3))\nplot_confusion_matrix(linear_svc, Xtest_tfidf, ytest,cmap = 'Blues')\nplt.grid(False)","metadata":{"execution":{"iopub.status.busy":"2021-07-23T19:33:10.953488Z","iopub.execute_input":"2021-07-23T19:33:10.953877Z","iopub.status.idle":"2021-07-23T19:33:13.200135Z","shell.execute_reply.started":"2021-07-23T19:33:10.953836Z","shell.execute_reply":"2021-07-23T19:33:13.199292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### XGboost ","metadata":{"execution":{"iopub.status.busy":"2021-07-23T07:04:08.496531Z","iopub.execute_input":"2021-07-23T07:04:08.496879Z","iopub.status.idle":"2021-07-23T07:06:36.237208Z","shell.execute_reply.started":"2021-07-23T07:04:08.496829Z","shell.execute_reply":"2021-07-23T07:06:36.235223Z"}}},{"cell_type":"code","source":"xgbo = XGBClassifier()\nxgbo.fit(Xtrain_tfidf,ytrain)\np4=xgbo.predict(Xtest_tfidf)\ns4=accuracy_score(ytest,p4)\nprint(\"XGBoost Accuracy :\", \"{:.2f}%\".format(100*s4))\nplot_confusion_matrix(xgbo, Xtest_tfidf, ytest, cmap = 'Blues')\nplt.grid(False)","metadata":{"execution":{"iopub.status.busy":"2021-07-23T19:33:13.20165Z","iopub.execute_input":"2021-07-23T19:33:13.202312Z","iopub.status.idle":"2021-07-23T19:35:40.647592Z","shell.execute_reply.started":"2021-07-23T19:33:13.20227Z","shell.execute_reply":"2021-07-23T19:35:40.646565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **5. Predictive Modelling using Deep Learning** ","metadata":{}},{"cell_type":"code","source":"def plotLearningCurve(history,epochs):\n  epochRange = range(1,epochs+1)\n  fig , ax = plt.subplots(1,2,figsize = (10,5))\n  \n  ax[0].plot(epochRange,history.history['accuracy'],label = 'Training Accuracy')\n  ax[0].plot(epochRange,history.history['val_accuracy'],label = 'Validation Accuracy')\n  ax[0].set_title('Training and Validation accuracy')\n  ax[0].set_xlabel('Epoch')\n  ax[0].set_ylabel('Accuracy')\n  ax[0].legend()\n  ax[1].plot(epochRange,history.history['loss'],label = 'Training Loss')\n  ax[1].plot(epochRange,history.history['val_loss'],label = 'Validation Loss')\n  ax[1].set_title('Training and Validation loss')\n  ax[1].set_xlabel('Epoch')\n  ax[1].set_ylabel('Loss')\n  ax[1].legend()\n  fig.tight_layout()\n  plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-23T19:35:40.64918Z","iopub.execute_input":"2021-07-23T19:35:40.649697Z","iopub.status.idle":"2021-07-23T19:35:40.66595Z","shell.execute_reply.started":"2021-07-23T19:35:40.649657Z","shell.execute_reply":"2021-07-23T19:35:40.664845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#splitting into train and test\ndata_copy['review']=data_copy['review'].apply(clean_text,lemmatize = False)\n#converting target variable to numerical value\ndata_copy.sentiment = [ 1 if each == \"positive\" else 0 for each in data_copy.sentiment]\ntrain, test= train_test_split(data_copy, test_size=0.2, random_state=42)\nXtrain, ytrain = train['review'], train['sentiment']\nXtest, ytest = test['review'], test['sentiment']","metadata":{"execution":{"iopub.status.busy":"2021-07-23T19:35:40.667437Z","iopub.execute_input":"2021-07-23T19:35:40.667982Z","iopub.status.idle":"2021-07-23T19:36:29.571975Z","shell.execute_reply.started":"2021-07-23T19:35:40.667938Z","shell.execute_reply":"2021-07-23T19:36:29.57114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### LSTM","metadata":{}},{"cell_type":"code","source":"#set up the tokenizer\nMAX_VOCAB_SIZE = 10000\ntokenizer = Tokenizer(num_words = MAX_VOCAB_SIZE,oov_token=\"<oov>\")\ntokenizer.fit_on_texts(Xtrain)\nword_index = tokenizer.word_index\n#print(word_index)\nV = len(word_index)\nprint(\"Vocabulary of the dataset is : \",V)\n","metadata":{"execution":{"iopub.status.busy":"2021-07-23T19:36:29.573132Z","iopub.execute_input":"2021-07-23T19:36:29.573516Z","iopub.status.idle":"2021-07-23T19:36:34.201501Z","shell.execute_reply.started":"2021-07-23T19:36:29.573478Z","shell.execute_reply":"2021-07-23T19:36:34.200437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##create sequences of reviews\nseq_train = tokenizer.texts_to_sequences(Xtrain)\nseq_test =  tokenizer.texts_to_sequences(Xtest)","metadata":{"execution":{"iopub.status.busy":"2021-07-23T19:36:34.202789Z","iopub.execute_input":"2021-07-23T19:36:34.203346Z","iopub.status.idle":"2021-07-23T19:36:38.052882Z","shell.execute_reply.started":"2021-07-23T19:36:34.203296Z","shell.execute_reply":"2021-07-23T19:36:38.05193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#choice of maximum length of sequences\nseq_len_list = [len(i) for i in seq_train + seq_test]\n\n#if we take the direct maximum then\nmax_len=max(seq_len_list)\nprint('Maximum length of sequence in the list: {}'.format(max_len))","metadata":{"execution":{"iopub.status.busy":"2021-07-23T19:36:38.054231Z","iopub.execute_input":"2021-07-23T19:36:38.054622Z","iopub.status.idle":"2021-07-23T19:36:38.067639Z","shell.execute_reply.started":"2021-07-23T19:36:38.054585Z","shell.execute_reply":"2021-07-23T19:36:38.066521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# when setting the maximum length of sequence, variability around the average is used.\nmax_seq_len = np.mean(seq_len_list) + 2 * np.std(seq_len_list)\nmax_seq_len = int(max_seq_len)\nprint('Maximum length of the sequence when considering data only two standard deviations from average: {}'.format(max_seq_len))","metadata":{"execution":{"iopub.status.busy":"2021-07-23T19:36:38.069276Z","iopub.execute_input":"2021-07-23T19:36:38.069719Z","iopub.status.idle":"2021-07-23T19:36:38.109215Z","shell.execute_reply.started":"2021-07-23T19:36:38.069677Z","shell.execute_reply":"2021-07-23T19:36:38.108291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"perc_covered = np.sum(np.array(seq_len_list) < max_seq_len) / len(seq_len_list)*100\nprint('The above calculated number coveres approximately {} % of data'.format(np.round(perc_covered,2)))\n","metadata":{"execution":{"iopub.status.busy":"2021-07-23T19:36:38.110447Z","iopub.execute_input":"2021-07-23T19:36:38.110949Z","iopub.status.idle":"2021-07-23T19:36:38.128576Z","shell.execute_reply.started":"2021-07-23T19:36:38.110908Z","shell.execute_reply":"2021-07-23T19:36:38.127622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So we can use this number for our maxlen parameter.","metadata":{}},{"cell_type":"code","source":"#create padded sequences\npad_train=pad_sequences(seq_train,truncating = 'post', padding = 'pre',maxlen=max_seq_len)\npad_test=pad_sequences(seq_test,truncating = 'post', padding = 'pre',maxlen=max_seq_len)","metadata":{"execution":{"iopub.status.busy":"2021-07-23T19:36:38.130081Z","iopub.execute_input":"2021-07-23T19:36:38.130665Z","iopub.status.idle":"2021-07-23T19:36:39.503135Z","shell.execute_reply.started":"2021-07-23T19:36:38.130622Z","shell.execute_reply":"2021-07-23T19:36:39.502173Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Splitting training set for validation purposes\nXtrain,Xval,ytrain,yval=train_test_split(pad_train,ytrain,\n                                             test_size=0.2,random_state=10)","metadata":{"execution":{"iopub.status.busy":"2021-07-23T19:36:39.50535Z","iopub.execute_input":"2021-07-23T19:36:39.50578Z","iopub.status.idle":"2021-07-23T19:36:39.539439Z","shell.execute_reply.started":"2021-07-23T19:36:39.505739Z","shell.execute_reply":"2021-07-23T19:36:39.538576Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def lstm_model(Xtrain,Xval,ytrain,yval,V,D,maxlen,epochs):\n\n    print(\"----Building the model----\")\n    i = Input(shape=(maxlen,))\n    x = Embedding(V + 1, D,input_length = maxlen)(i)\n    x = BatchNormalization()(x)\n    x = Dropout(0.3)(x)\n    x = Conv1D(32,5,activation = 'relu')(x)\n    x = Dropout(0.3)(x)\n    x = MaxPooling1D(2)(x)\n    x = Bidirectional(LSTM(128,return_sequences=True))(x)\n    x = LSTM(64)(x)\n    x = Dropout(0.5)(x)\n    x = Dense(1, activation='sigmoid')(x)\n    model = Model(i, x)\n    model.summary()\n\n    #Training the LSTM\n    print(\"----Training the network----\")\n    model.compile(optimizer= Adam(0.0005),\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\n    \n#     #early_stop = EarlyStopping(monitor='val_accuracy', \n#                                mode='min', \n#                                patience = 2 )\n#     #checkpoints= ModelCheckpoint(filepath='./',\n#                             monitor=\"val_accuracy\",\n#                             verbose=0,\n#                             save_best_only=True\n#                            )\n  #  callbacks = [checkpoints,early_stop]\n    r = model.fit(Xtrain,ytrain, \n                  validation_data = (Xval,yval), \n                  epochs = epochs, \n                  verbose = 2,\n                  batch_size = 32)\n                  #callbacks = callbacks\n    print(\"Train score:\", model.evaluate(Xtrain,ytrain))\n    print(\"Validation score:\", model.evaluate(Xval,yval))\n    n_epochs = len(r.history['loss'])\n    \n    return r,model,n_epochs \n","metadata":{"execution":{"iopub.status.busy":"2021-07-23T19:36:39.54094Z","iopub.execute_input":"2021-07-23T19:36:39.541347Z","iopub.status.idle":"2021-07-23T19:36:39.553529Z","shell.execute_reply.started":"2021-07-23T19:36:39.541306Z","shell.execute_reply":"2021-07-23T19:36:39.552514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"D = 64 #embedding dims\nepochs = 5\nr,model,n_epochs = lstm_model(Xtrain,Xval,ytrain,yval,V,D,max_seq_len,epochs)","metadata":{"execution":{"iopub.status.busy":"2021-07-23T19:36:39.554921Z","iopub.execute_input":"2021-07-23T19:36:39.555289Z","iopub.status.idle":"2021-07-23T19:49:41.330319Z","shell.execute_reply.started":"2021-07-23T19:36:39.55525Z","shell.execute_reply":"2021-07-23T19:49:41.329276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Plot accuracy and loss\nplotLearningCurve(r,n_epochs)","metadata":{"execution":{"iopub.status.busy":"2021-07-23T19:49:41.331711Z","iopub.execute_input":"2021-07-23T19:49:41.332079Z","iopub.status.idle":"2021-07-23T19:49:41.807575Z","shell.execute_reply.started":"2021-07-23T19:49:41.332038Z","shell.execute_reply":"2021-07-23T19:49:41.806419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Evaluate Model Performance on Test set\")\nresult = model.evaluate(pad_test,ytest)\nprint(dict(zip(model.metrics_names, result)))","metadata":{"execution":{"iopub.status.busy":"2021-07-23T19:49:41.80908Z","iopub.execute_input":"2021-07-23T19:49:41.809477Z","iopub.status.idle":"2021-07-23T19:49:45.387491Z","shell.execute_reply.started":"2021-07-23T19:49:41.809439Z","shell.execute_reply":"2021-07-23T19:49:45.37852Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Generate predictions for the test dataset\nypred = model.predict(pad_test)\nypred = ypred>0.5\n#Get the confusion matrix\ncf_matrix = confusion_matrix(ytest, ypred)\nsns.heatmap(cf_matrix,annot = True,fmt ='g', cmap='Blues')\nplt.xlabel('Predicted label')\nplt.ylabel('True label')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-23T19:49:45.389257Z","iopub.execute_input":"2021-07-23T19:49:45.389684Z","iopub.status.idle":"2021-07-23T19:49:49.531593Z","shell.execute_reply.started":"2021-07-23T19:49:45.389636Z","shell.execute_reply":"2021-07-23T19:49:49.530555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### BERT\n","metadata":{}},{"cell_type":"code","source":"train, test= train_test_split(data_copy, test_size=0.2, random_state=42)\nXtrain, ytrain = train['review'], train['sentiment']\nXtest, ytest = test['review'], test['sentiment']\n#splitting the train set into train and validation\nXtrain,Xval,ytrain,yval=train_test_split(Xtrain,ytrain,\n                                             test_size=0.2,random_state=10)","metadata":{"execution":{"iopub.status.busy":"2021-07-23T19:49:49.533424Z","iopub.execute_input":"2021-07-23T19:49:49.534146Z","iopub.status.idle":"2021-07-23T19:49:49.562004Z","shell.execute_reply.started":"2021-07-23T19:49:49.534099Z","shell.execute_reply":"2021-07-23T19:49:49.561238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Perform tokenization\n# automatically download the vocab used during pretraining or fine-tuning a given model,use from_pretrained() method\ntokenizer = transformers.AutoTokenizer.from_pretrained('distilbert-base-uncased')\n","metadata":{"execution":{"iopub.status.busy":"2021-07-23T19:49:49.563452Z","iopub.execute_input":"2021-07-23T19:49:49.563819Z","iopub.status.idle":"2021-07-23T19:49:51.846405Z","shell.execute_reply.started":"2021-07-23T19:49:49.563782Z","shell.execute_reply":"2021-07-23T19:49:51.84559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#pass our texts to the tokenizer. \nXtrain_enc = tokenizer(Xtrain.tolist(), max_length=max_seq_len, \n                         truncation=True, padding='max_length', \n                         add_special_tokens=True, return_tensors='np') #return numpy object\nXval_enc = tokenizer(Xval.tolist(), max_length=max_seq_len, \n                         truncation=True, padding='max_length', \n                         add_special_tokens=True, return_tensors='np') #return numpy object\nXtest_enc = tokenizer(Xtest.tolist(), max_length=max_seq_len, \n                         truncation=True, padding='max_length', \n                         add_special_tokens=True, return_tensors='np') #return numpy object\n\n\n","metadata":{"execution":{"iopub.status.busy":"2021-07-23T19:49:51.847524Z","iopub.execute_input":"2021-07-23T19:49:51.847912Z","iopub.status.idle":"2021-07-23T19:50:21.536884Z","shell.execute_reply.started":"2021-07-23T19:49:51.847882Z","shell.execute_reply":"2021-07-23T19:50:21.536009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#preparing our datasets\ntrain_dataset = tf.data.Dataset.from_tensor_slices((\n    dict(Xtrain_enc),\n    ytrain\n))\nval_dataset = tf.data.Dataset.from_tensor_slices((\n    dict(Xval_enc),\n    yval\n))\ntest_dataset = tf.data.Dataset.from_tensor_slices((\n    dict(Xtest_enc),\n    ytest\n))","metadata":{"execution":{"iopub.status.busy":"2021-07-23T19:50:21.53815Z","iopub.execute_input":"2021-07-23T19:50:21.538519Z","iopub.status.idle":"2021-07-23T19:50:21.69645Z","shell.execute_reply.started":"2021-07-23T19:50:21.53848Z","shell.execute_reply":"2021-07-23T19:50:21.69543Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def bert_model(train_dataset,val_dataset,transformer,max_len,epochs):\n    print(\"----Building the model----\")\n    input_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_ids\")\n    attention_mask = Input(shape=(max_len,),dtype=tf.int32,name = 'attention_mask') #attention mask\n    sequence_output = transformer(input_ids,attention_mask)[0]\n    cls_token = sequence_output[:, 0, :]\n    x = Dense(512, activation='relu')(cls_token)\n    x = Dropout(0.1)(x)\n    y = Dense(1, activation='sigmoid')(x)\n    model = Model(inputs=[input_ids,attention_mask], outputs=y)\n    model.summary()\n    model.compile(Adam(lr=2e-5), loss='binary_crossentropy', metrics=['accuracy'])\n    r = model.fit(train_dataset.batch(32),batch_size = 32,\n                  validation_data = val_dataset.batch(32),epochs = epochs)\n                  #callbacks = callbacks\n    print(\"Train score:\", model.evaluate(train_dataset.batch(32)))\n    print(\"Validation score:\", model.evaluate(val_dataset.batch(32)))\n    n_epochs = len(r.history['loss'])\n    \n    return r,model,n_epochs ","metadata":{"execution":{"iopub.status.busy":"2021-07-23T19:50:21.697832Z","iopub.execute_input":"2021-07-23T19:50:21.698212Z","iopub.status.idle":"2021-07-23T19:50:21.709509Z","shell.execute_reply.started":"2021-07-23T19:50:21.698172Z","shell.execute_reply":"2021-07-23T19:50:21.708597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transformer = transformers.TFDistilBertModel.from_pretrained('distilbert-base-uncased')","metadata":{"execution":{"iopub.status.busy":"2021-07-23T19:50:21.710795Z","iopub.execute_input":"2021-07-23T19:50:21.711391Z","iopub.status.idle":"2021-07-23T19:50:22.996376Z","shell.execute_reply.started":"2021-07-23T19:50:21.711349Z","shell.execute_reply":"2021-07-23T19:50:22.995581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs = 2\nmax_len = max_seq_len\nr,model,n_epochs = bert_model(train_dataset,val_dataset,transformer,max_len,epochs)","metadata":{"execution":{"iopub.status.busy":"2021-07-23T19:50:22.997683Z","iopub.execute_input":"2021-07-23T19:50:22.998229Z","iopub.status.idle":"2021-07-23T20:13:41.997427Z","shell.execute_reply.started":"2021-07-23T19:50:22.998187Z","shell.execute_reply":"2021-07-23T20:13:41.996266Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Plot accuracy and loss\nplotLearningCurve(r,n_epochs)","metadata":{"execution":{"iopub.status.busy":"2021-07-23T20:13:41.999399Z","iopub.execute_input":"2021-07-23T20:13:41.999786Z","iopub.status.idle":"2021-07-23T20:13:42.520594Z","shell.execute_reply.started":"2021-07-23T20:13:41.999746Z","shell.execute_reply":"2021-07-23T20:13:42.519505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Evaluate Model Performance on Test set\")\nresult = model.evaluate(test_dataset.batch(32))\nprint(dict(zip(model.metrics_names, result)))","metadata":{"execution":{"iopub.status.busy":"2021-07-23T20:13:42.522284Z","iopub.execute_input":"2021-07-23T20:13:42.522738Z","iopub.status.idle":"2021-07-23T20:14:38.201348Z","shell.execute_reply.started":"2021-07-23T20:13:42.522627Z","shell.execute_reply":"2021-07-23T20:14:38.200231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Generate predictions for the test dataset\nypred = model.predict(test_dataset.batch(32))\nypred = ypred>0.5\n#Get the confusion matrix\ncf_matrix = confusion_matrix(ytest, ypred)\nsns.heatmap(cf_matrix,annot = True,fmt ='g', cmap='Blues')\nplt.xlabel('Predicted label')\nplt.ylabel('True label')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-23T20:14:38.202753Z","iopub.execute_input":"2021-07-23T20:14:38.203239Z","iopub.status.idle":"2021-07-23T20:15:33.617508Z","shell.execute_reply.started":"2021-07-23T20:14:38.203175Z","shell.execute_reply":"2021-07-23T20:15:33.616657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### ","metadata":{}}]}